{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "\n",
    "__author__ = \"Amir Yeganehsahab\"\n",
    "__license__ = \"MIT\"\n",
    "__version__ = \"2023-11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x_y_data_v1(isTrain, batch_size):\n",
    "    \"\"\"\n",
    "    Data for exercise 1.\n",
    "\n",
    "    returns: tuple (X, Y)\n",
    "        X is a sine and a cosine from 0.0*pi to 1.5*pi\n",
    "        Y is a sine and a cosine from 1.5*pi to 3.0*pi\n",
    "    Therefore, Y follows X. There is also a random offset\n",
    "    commonly applied to X an Y.\n",
    "\n",
    "    The returned arrays are of shape:\n",
    "        (seq_length, batch_size, output_dim)\n",
    "        Therefore: (10, batch_size, 2)\n",
    "\n",
    "    For this exercise, let's ignore the \"isTrain\"\n",
    "    argument and test on the same data.\n",
    "    \"\"\"\n",
    "    seq_length = 10\n",
    "\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for _ in range(batch_size):\n",
    "        rand = random.random() * 2 * math.pi\n",
    "\n",
    "        sig1 = np.sin(np.linspace(0.0 * math.pi + rand,\n",
    "                                  3.0 * math.pi + rand, seq_length * 2))\n",
    "        sig2 = np.cos(np.linspace(0.0 * math.pi + rand,\n",
    "                                  3.0 * math.pi + rand, seq_length * 2))\n",
    "        x1 = sig1[:seq_length]\n",
    "        y1 = sig1[seq_length:]\n",
    "        x2 = sig2[:seq_length]\n",
    "        y2 = sig2[seq_length:]\n",
    "\n",
    "        x_ = np.array([x1, x2])\n",
    "        y_ = np.array([y1, y2])\n",
    "        x_, y_ = x_.T, y_.T\n",
    "\n",
    "        batch_x.append(x_)\n",
    "        batch_y.append(y_)\n",
    "\n",
    "    batch_x = np.array(batch_x)\n",
    "    batch_y = np.array(batch_y)\n",
    "    # shape: (batch_size, seq_length, output_dim)\n",
    "\n",
    "    batch_x = np.array(batch_x).transpose((1, 0, 2))\n",
    "    batch_y = np.array(batch_y).transpose((1, 0, 2))\n",
    "    # shape: (seq_length, batch_size, output_dim)\n",
    "\n",
    "    return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x_y_data_two_freqs(isTrain, batch_size, seq_length):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for _ in range(batch_size):\n",
    "        offset_rand = random.random() * 2 * math.pi\n",
    "        freq_rand = (random.random() - 0.5) / 1.5 * 15 + 0.5\n",
    "        amp_rand = random.random() + 0.1\n",
    "\n",
    "        sig1 = amp_rand * np.sin(np.linspace(\n",
    "            seq_length / 15.0 * freq_rand * 0.0 * math.pi + offset_rand,\n",
    "            seq_length / 15.0 * freq_rand * 3.0 * math.pi + offset_rand,\n",
    "            seq_length * 2\n",
    "        )\n",
    "        )\n",
    "\n",
    "        offset_rand = random.random() * 2 * math.pi\n",
    "        freq_rand = (random.random() - 0.5) / 1.5 * 15 + 0.5\n",
    "        amp_rand = random.random() * 1.2\n",
    "\n",
    "        sig1 = amp_rand * np.cos(np.linspace(\n",
    "            seq_length / 15.0 * freq_rand * 0.0 * math.pi + offset_rand,\n",
    "            seq_length / 15.0 * freq_rand * 3.0 * math.pi + offset_rand,\n",
    "            seq_length * 2\n",
    "        )\n",
    "        ) + sig1\n",
    "\n",
    "        x1 = sig1[:seq_length]\n",
    "        y1 = sig1[seq_length:]\n",
    "\n",
    "        x_ = np.array([x1])\n",
    "        y_ = np.array([y1])\n",
    "        x_, y_ = x_.T, y_.T\n",
    "\n",
    "        batch_x.append(x_)\n",
    "        batch_y.append(y_)\n",
    "\n",
    "    batch_x = np.array(batch_x)\n",
    "    batch_y = np.array(batch_y)\n",
    "    # shape: (batch_size, seq_length, output_dim)\n",
    "\n",
    "    batch_x = np.array(batch_x).transpose((1, 0, 2))\n",
    "    batch_y = np.array(batch_y).transpose((1, 0, 2))\n",
    "    # shape: (seq_length, batch_size, output_dim)\n",
    "\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x_y_data_v2(isTrain, batch_size):\n",
    "    \"\"\"\n",
    "    Similar the the \"v1\" function, but here we generate a signal with\n",
    "    2 frequencies chosen randomly - and this for the 2 signals. Plus,\n",
    "    the lenght of the examples is of 15 rather than 10.\n",
    "    So we have 30 total values for past and future.\n",
    "    \"\"\"\n",
    "    return generate_x_y_data_two_freqs(isTrain, batch_size, seq_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x_y_data_v3(isTrain, batch_size):\n",
    "    \"\"\"\n",
    "    Similar to the \"v2\" function, but here we generate a signal\n",
    "    with noise in the X values. Plus,\n",
    "    the lenght of the examples is of 30 rather than 10.\n",
    "    So we have 60 total values for past and future.\n",
    "    \"\"\"\n",
    "    seq_length = 30\n",
    "    x, y = generate_x_y_data_two_freqs(\n",
    "        isTrain, batch_size, seq_length=seq_length)\n",
    "    noise_amount = random.random() * 0.15 + 0.10\n",
    "    x = x + noise_amount * np.random.randn(seq_length, batch_size, 1)\n",
    "\n",
    "    avg = np.average(x)\n",
    "    std = np.std(x) + 0.0001\n",
    "    x = x - avg\n",
    "    y = y - avg\n",
    "    x = x / std / 2.5\n",
    "    y = y / std / 2.5\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCurrency(curr, window_size):\n",
    "    \"\"\"\n",
    "    Return the historical data for the USD or EUR bitcoin value. Is done with an web API call.\n",
    "    curr = \"USD\" | \"EUR\"\n",
    "    \"\"\"\n",
    "    # For more info on the URL call, it is inspired by :\n",
    "    # https://github.com/Levino/coindesk-api-node\n",
    "    r = requests.get(\n",
    "        \"http://api.coindesk.com/v1/bpi/historical/close.json?start=2010-07-17&end=2017-03-03&currency={}\".format(\n",
    "            curr\n",
    "        )\n",
    "    )\n",
    "    data = r.json()\n",
    "    time_to_values = sorted(data[\"bpi\"].items())\n",
    "    values = [val for key, val in time_to_values]\n",
    "    kept_values = values[1000:]\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(kept_values) - window_size * 2):\n",
    "        X.append(kept_values[i:i + window_size])\n",
    "        Y.append(kept_values[i + window_size:i + window_size * 2])\n",
    "\n",
    "    # To be able to concat on inner dimension later on:\n",
    "    X = np.expand_dims(X, axis=2)\n",
    "    Y = np.expand_dims(Y, axis=2)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, Y=None):\n",
    "    \"\"\"\n",
    "    Normalise X and Y according to the mean and standard deviation of the X values only.\n",
    "    \"\"\"\n",
    "    # # It would be possible to normalize with last rather than mean, such as:\n",
    "    # lasts = np.expand_dims(X[:, -1, :], axis=1)\n",
    "    # assert (lasts[:, :] == X[:, -1, :]).all(), \"{}, {}, {}. {}\".format(lasts[:, :].shape, X[:, -1, :].shape, lasts[:, :], X[:, -1, :])\n",
    "    mean = np.expand_dims(np.average(X, axis=1) + 0.00001, axis=1)\n",
    "    stddev = np.expand_dims(np.std(X, axis=1) + 0.00001, axis=1)\n",
    "    # print (mean.shape, stddev.shape)\n",
    "    # print (X.shape, Y.shape)\n",
    "    X = X - mean\n",
    "    X = X / (2.5 * stddev)\n",
    "    if Y is not None:\n",
    "        assert Y.shape == X.shape, (Y.shape, X.shape)\n",
    "        Y = Y - mean\n",
    "        Y = Y / (2.5 * stddev)\n",
    "        return X, Y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch_size_random(X, Y, batch_size):\n",
    "    \"\"\"\n",
    "    Returns randomly an aligned batch_size of X and Y among all examples.\n",
    "    The external dimension of X and Y must be the batch size (eg: 1 column = 1 example).\n",
    "    X and Y can be N-dimensional.\n",
    "    \"\"\"\n",
    "    assert X.shape == Y.shape, (X.shape, Y.shape)\n",
    "    idxes = np.random.randint(X.shape[0], size=batch_size)\n",
    "    X_out = np.array(X[idxes]).transpose((1, 0, 2))\n",
    "    Y_out = np.array(Y[idxes]).transpose((1, 0, 2))\n",
    "    return X_out, Y_out\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x_y_data_v4(isTrain, batch_size):\n",
    "    \"\"\"\n",
    "    Return financial data for the bitcoin.\n",
    "\n",
    "    Features are USD and EUR, in the internal dimension.\n",
    "    We normalize X and Y data according to the X only to not\n",
    "    spoil the predictions we ask for.\n",
    "\n",
    "    For every window (window or seq_length), Y is the prediction following X.\n",
    "    Train and test data are separated according to the 80/20 rule.\n",
    "    Therefore, the 20 percent of the test data are the most\n",
    "    recent historical bitcoin values. Every example in X contains\n",
    "    40 points of USD and then EUR data in the feature axis/dimension.\n",
    "    It is to be noted that the returned X and Y has the same shape\n",
    "    and are in a tuple.\n",
    "    \"\"\"\n",
    "    # 40 pas values for encoder, 40 after for decoder's predictions.\n",
    "    seq_length = 40\n",
    "\n",
    "    global Y_train\n",
    "    global X_train\n",
    "    global X_test\n",
    "    global Y_test\n",
    "    # First load, with memoization:\n",
    "    if len(Y_test) == 0:\n",
    "        # API call:\n",
    "        X_usd, Y_usd = loadCurrency(\"USD\", window_size=seq_length)\n",
    "        X_eur, Y_eur = loadCurrency(\"EUR\", window_size=seq_length)\n",
    "\n",
    "        # All data, aligned:\n",
    "        X = np.concatenate((X_usd, X_eur), axis=2)\n",
    "        Y = np.concatenate((Y_usd, Y_eur), axis=2)\n",
    "        X, Y = normalize(X, Y)\n",
    "\n",
    "        # Split 80-20:\n",
    "        X_train = X[:int(len(X) * 0.8)]\n",
    "        Y_train = Y[:int(len(Y) * 0.8)]\n",
    "        X_test = X[int(len(X) * 0.8):]\n",
    "        Y_test = Y[int(len(Y) * 0.8):]\n",
    "\n",
    "    if isTrain:\n",
    "        return fetch_batch_size_random(X_train, Y_train, batch_size)\n",
    "    else:\n",
    "        return fetch_batch_size_random(X_test,  Y_test,  batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise = 1  # Possible values: 1, 2, 3, or 4.\n",
    "\n",
    "# We choose which data function to use below, in function of the exericse. \n",
    "if exercise == 1:\n",
    "    generate_x_y_data = generate_x_y_data_v1\n",
    "if exercise == 2:\n",
    "    generate_x_y_data = generate_x_y_data_v2\n",
    "if exercise == 3:\n",
    "    generate_x_y_data = generate_x_y_data_v3\n",
    "if exercise == 4:  \n",
    "    generate_x_y_data = generate_x_y_data_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20626/3648286850.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m  \u001b[0;31m# Version 1.0 or 0.12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# This is for the notebook to generate inline matplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # Version 1.0 or 0.12\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is for the notebook to generate inline matplotlib \n",
    "# charts rather than to open a new window every time: \n",
    "%matplotlib inline\n",
    "https://www.kaggle.com/code/gchevalier/signal-prediction-with-a-seq2seq-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
